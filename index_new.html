<!doctype html>
<html lang="en">
  <head>
    <link href="captionss/captionss.css" rel="stylesheet" type="text/css">
    <meta charset="utf-8">
    <title>Supervoxels - Temporal Voxel Manifolds and Applications</title>
    <meta name="author" content="Jérémie Papon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="reveal/css/reveal.min.css">
    <link rel="stylesheet" href="pkgwtheme.css" id="theme">
    <link rel="stylesheet" href="reveal/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <!--[if lt IE 9]>
    <script src="reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
    
    <!--Adobe Edge Runtime-->
    <meta http-equiv="X-UA-Compatible" content="IE=Edge">
    <script type="text/javascript" charset="utf-8" src="edge_includes/edge.5.0.0.min.js"></script>
    <style>
        .edgeLoad-EDGE-3489513 { visibility:hidden; }
    </style>
    <!--Load HTVF animation -->
    <script>
      AdobeEdge.loadComposition('HTVF_anim2', 'EDGE-3489513', {
        scaleToFit: "none",
        centerStage: "both",
        minW: "0",
        maxW: "undefined",
        width: "1024px",
        height: "625px"
    }, {dom: [ ]}, {dom: [ ]});
    </script>
    <!--Adobe Edge Runtime End-->

  </head>

  <body>
    <div class="reveal">
      <resetfootnotecounter></resetfootnotecounter>
      <resetcitationcounter></resetcitationcounter>

      <div class="slides">
        <section>
          <p style="text-align: center"><br><br><br>Press F11 to view this in full screen.</p>
          <p style="text-align: center">Press Left/Right to advance through the presentation.</p>
          <p style="text-align: center">Make sure to click on the play button for Point Clouds!</p>
          <p style="text-align: center">Don't miss the vertical slides - you'll see up/down arrows on the bottom right!</p>
          <p style="text-align: center">You can press the "esc" key to go to a slide overview.</p>
          <br>
          <p style="text-align: center"> For more information on this work, visit <a href="http://www.jeremiepapon.com/">http://www.jeremiepapon.com/ </a></p>
        </section>

        
        <section data-background="data/Buildings_Cropped_Merge.png">
          <h1 style="text-shadow:4px 4px #000000">Supervoxels</h1>
          <h3 style="text-shadow:3px 3px #000000">Temporal Voxel Manifolds and their Applications</h3>
          <footer style="font-size: 36px">             
          <div> <a style="color: #00FF00;text-shadow:4px 4px #000000" href="http://www.jeremiepapon.com/"><strong>Jérémie Papon</strong></a> </div>
          <div style="text-shadow:3px 3px #000000">
            Georg-August-Universität Göttingen <br>
            Bernstein Center for Computational Neuroscience <br>
            Institut für Informatik <br>
            5 March 2015
          </div>
          </footer>
          
        </section>

        <!-- MOTIVATION -->
        <section>
          <h2>How do we learn to perceive objects?</h2>
          <blockquote style="font-size: 1.2em; line-height: 125%; background:">“Infants appear to perceive objects by analyzing <span class="fragment highlight-green">three-dimensional surface arrangements and motions </span>... [they] divide perceptual arrays into <span class="fragment highlight-red">units that move as connected wholes</span>, <span class="fragment highlight-blue">that move separately from one another</span>, that tend to <span class="fragment highlight-green">maintain their size and shape over motion</span>, and that tend to <span class="fragment highlight-red">act upon each other only on contact.</span>” *</blockquote>
          <br>
          
          <div class="w50" style="float: right">
              <img src="data/child_colors.jpg" width="70%">
          </div>
          <div class="w50">
            <video src="data/movies/cutting.mp4" width="75%" controls loop class="slideautostart"></video>
          </div>
          <p> How can we track these units without a-priori object knowledge? </p>
           <div class='footer' >
              * Spelke, Elizabeth S. "Principles of object perception." Cognitive science 14, no. 1 (1990): 29-56.
           </div>
            
        </section> 
        
        <section>
          <h2>Temporal Connections without Objects</h2>
          <p>How can we create partitions when we don't know what an object is before-hand?</p>
          <video src="data/movies/cutting.mp4" width="70%" controls loop class="slideautostart"></video>
          <p>We have no difficulty tracking the pieces of objects when they split.</p>
          <ul>
          <li>This implies maintenance of both low-level and object-level spatio-temporal tracking.</li>
          </ul>
        </section>

        <!-- Existing Stuff -->
        <section>
          <section>
            <h2>Parsing Video Streams -<br> Existing Methodologies</h2>
            <p><strong>Video Object Segmentation</strong> 
                e.g.Abramov et al.<cite></cite>
                Grundmann et al.<cite></cite></p>
            <p>This parses a video into spatio-temporal volumes - “objects”</p>
            <p>Core assumption means that “objects” must form <span class="fragment highlight-green">continuous spatio-temporal</span> volumes!</p>
            <video src="data/movies/grundmann/occlusion_grund03_append.mp4" height="60%" controls loop class="slideautostart"></video>
            <p class="rcred"><a href="http://www.videosegmentation.com/">Processed on VideoSegmentation.com</a></p>
            <br>
            <div class='citation'>
              <footl>
                <footi>Abramov et al., <a href="http://dx.doi.org/10.1109/TCSVT.2012.2199389">Real-Time Segmentation of Stereo Videos on a Portable System With a Mobile GPU,</a> <em>IEEE Transactions on Circuits and Systems for Video Technology </em>2012.</footi>
                <footi>Grundmann et al., <a href="http://www.cc.gatech.edu/cpl/projects/videosegmentation/cvpr2010_videosegmentation.pdf">Efficient Hierarchical Graph Based Video Segmentation,</a><em>Computer Vision and Pattern Recognition (CVPR)</em> 2010.</footi>
              </footl>
            </div>
          </section>
        
          <section>
            <h2>Parsing Video Streams -<br> Existing Methodologies</h2>
            <video src="data/movies/grundmann/cutting_grund03.mp4" height="70%" controls loop class="slideautostart"></video>
            <p class="rcred"><a href="http://www.videosegmentation.com/">Processed on VideoSegmentation.com</a></p>
            <div>Complete failure if this assumption is violated. </div>
          </section>
        </section>
        
        <section>
          <h2>Parsing Video Streams -<br> Existing Methodologies</h2>
          <p><strong>Semantic Event Chains</strong><cite></cite> - Represents by analyzing creation & deletion of edges in segment adjacency graph.</p>

          <p>Analysis of temporal evolution of graph structure yields semantics</p>
          <video src="data/movies/eren/maniac_breakfast.mp4" height="60%" controls loop class="slideautostart"></video>
          <p class="rcred"><a href="https://www.youtube.com/watch?v=Gt5TVEcSTTE">Maniac Dataset: Breakfast</a></p>
          <div>This requires  <span class="fragment highlight-green">a-priori knowledge</span> of objects!</div>
          <br>
          <div class='citation'>
            <footl>
              <footi>Aksoy, Eren Erdal, et al. <a href="http://www.dpi.physik.uni-goettingen.de/~eaksoye/papers/IJRR_2011.pdf">Learning the semantics of object–action relations by observation.</a> <em>The International Journal of Robotics Research</em> (2011).</footi>
            </footl>
          </div>
        </section>
        
        <!-- 
        <section style="line-height: 135%">
          <h2>Transitioning to 3D</h2>
          <p>To Summarize most vexing issues </p>
          <ul>
            <li class="fragment">Segment into spatio-temporal volumes - <span class="fragment current-visible">cannot handle occlusions</span></li>
            <li class="fragment">Divide the scene into objects <em><span class="fragment highlight-green">before</span></em> observations.</li>
            <ul style="list-style-type: none"><li class="fragment">Cannot learn “object-ness” from observations</li></ul>
            <li class="fragment"> Only use color - 3D geometry is not considered </li>
          </ul>
          <p class="fragment">To overcome some of these, we use RGB-D sensors to capture Point Clouds</p>
          <div class="ctr w70">
          <figure class="embed hide-smooth dark" >
            <img src="data/openni_cams.jpg">
            <figcaption style="font-size:0.75em">
              Some OpenNI Sensors which capture RGB+D data.
            </figcaption>
          </figure>
          </div>
          <p class="rcred"><a href="http://www.pointclouds.org/">Point Cloud Library (PCL)</a></p>
        </section>  
        -->
        <section>
          <h2>Overview of Methodology</h2>
          <div>
           <img src="data/Overall_Cropped.svg" width="100%">
           </div>
          <div style="position:absolute;left:785px;top:137px;">
            <video src="data/movies/cutting_noocc_result_png.mp4" height="50%" loop class="slideautostart"></video>
          </div>
          
        </section>
        
        <section>
          <h2>A Point Cloud</h2>
          <div align="center">
            <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://jpapon.github.io/data/pointclouds/cutting_demo_cloud.pcd&psize=3" width="1100" height="650" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;">
            </iframe>
          </div>
          <p>Advantages of 3D</p>
          <ul>
            <li> Avoids size/shape ambiguities of perspective transformation. </li>
            <li> Can reason about occlusions at a low level. </li>
            <li> Can use size and shape as a feature. </li>
          </ul>
        </section>
        
        <!--  
        <section>
          <h2>Octree Voxelization</h2>
          <div class="w45" style="float: right">
            <img src="data/bunnywork.png">
            <p class="rcred"><a href="http://www.pointclouds.org/">Point Cloud Library (PCL)</a></p>
          </div>
          <p>Insert pointcloud into a grid of cubic voxels.</p>
          <p>Represent all points in one cell by its centroid.</p>
          <p>$$\vec{ \overline{p}} = \frac{1}{N_i}\sum_i \vec p_i$$</p>
          <p>The edge length $L_\text{voxel}$ of voxels defines scale of observation and determines octree minimum bin size.</p>
        </section>
        
        <section>
          <h2>A Voxelized Point Cloud</h2>
          <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://134.76.92.76/data/cutting_demo_cloud_voxel_trans.pcd&psize=2" width="1100" height="750" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="">
          </iframe>
        </section>
        -->
        
        <section>
          <h2>Building an Adjacency Graph</h2>
            <ul>
              <li class="fragment">Special octree type developed which maintains adjacency information of voxels</li>
              <li class="fragment">This gives us back pixel-like (grid) relations, while keeping real 3D adjacency</li>
              <li class="fragment">Region growing and connectivity graph become very efficient</li>
            </ul>
          <div class="ctr w80">
            <figure class="embed hide-smooth dark" >
              <img src="data/AdjacencyOctree.svg">
              <figcaption style="font-size:1.0em">
                Octree Adjacency Structure - Leaves now link to their spatial neighbors.
              </figcaption>
            </figure>
          </div>
        </section>
        
        <section>
          <h2>Voxel Cloud Connectivity Segmentation</h2>
          <ul>
            <li class="fragment">VCCS <cite></cite> is a region-growing oversegmentation technique that uses local geometry to respect object boundaries</li>
            <li class="fragment">Constrained to flow across voxel connections</li>
            <li class="fragment">Use color, normals, and a spatial smoothness constraint </li>
          </ul>
        
          <div style="width:50%; float:left">
            <figure class="embed hide-smooth dark" >
              <img src="data/test55.png"">
              <figcaption style="font-size:1.0em">
                Test Scene
              </figcaption>
            </figure>
          </div>

          <div style="width:50%; float:right">
            <figure class="embed hide-smooth dark" >
              <img src="data/supervoxel-growth.gif">
              <figcaption style="font-size:1.0em">
                Iterative Expansion of Supervoxels using VCCS
              </figcaption>
            </figure>
          <div>
          <p class="rcred"><a href="http://users.acin.tuwien.ac.at/arichtsfeld/?site=4">OSD Dataset</a> <a href="http://www.pointclouds.org/blog/tocs/alexandrov/index.php">Sergey Alexandrov</a></p>
          <br>
          <div class='citation'>
            <footl>
              <footi>Papon et al., <a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2013.</footi>
            </footl>
          </div>
        </section>
        
        <section>
              <section>
                <h2>Examples of Supervoxels</h2>
                <p> Example of Supervoxels with different seed sizes - from NYU Dataset <cite></cite> </p>
                <div class="ctr w100">
                  <img src="data/IncreasingSeedSizePlain.svg" width="100%" >
                  <p class="rcred"><a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Papon et al. CVPR 2013</a></p>
                </div>
                <div class="ctr" style="width:75%">
                <figure class="embed-top hide-smooth dark" >
                  <img src="data/VCCS_Performance.svg">
                  <figcaption style="font-size:1.0em">
                    Performance of VCCS Compared to state of the art methods
                  </figcaption>
                </figure>
                </div>  
                <br>
                <div class="citation">
                  <footl>
                    <footi>Silberman et al., <a href="http://cs.nyu.edu/~silberman/projects/indoor_scene_seg_sup.html">Indoor Segmentation and Support Inference from RGBD Images,</a> European Conference on Computer Vision (ECCV) 2012.</footi>
                  </footl>
                </div>
              </section>
              
              <section>
                <h2>Quantitative Comparison to SLIC</h2>
                <div>
                  <figure class="embed-top hide-smooth dark" >
                    <img src="data/IncreasingSeedSizePlain.svg">
                    <figcaption style="font-size:1.0em">
                      VCCS Supervoxels for increasing seed size.
                    </figcaption>
                  </figure>
                  <p class="rcred"><a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Papon et al. CVPR 2013</a></p>
                </div>
                <div>
                  <figure class="embed-top hide-smooth dark" >
                    <img src="data/ComparisonToSLIC.svg">
                    <figcaption style="font-size:1.0em">
                      SLIC Superpixels
                    </figcaption>
                  </figure>
                </div>  
                <br>
                <div class="citation" >
                  <footl>
                    <footi>Achanta et al., <a href="http://ivrg.epfl.ch/research/superpixels">SLIC Superpixels Compared to State-of-the-art Superpixel Methods, </a> IEEE Transactions on Pattern Analysis and Machine Intelligence, 2012.</footi>
                  </footl>
                </div>
              </section>
              
              <section> 
                <h2> Speed and Performance vs State of the Art </h2>
                <div class="ctr" style="width:80%">
                  <figure class="embed-top hide-smooth dark" >
                    <img src="data/VCCS_Performance.svg">
                    <figcaption style="font-size:1.0em">
                      Performance of VCCS Compared to state of the art methods
                    </figcaption>
                  </figure>
                </div>  
                <div class="ctr" style="width:80%">
                  <figure class="embed-top hide-smooth dark" >
                    <img src="data/VCCS_Speed.svg">
                    <figcaption style="font-size:1.0em">
                      Speed of VCCS Compared to state of the art methods
                    </figcaption>
                  </figure>
                </div> 
              </section> 
              
        </section>
        
        <section>
          <h2>Supervoxels in a Point Cloud</h2>
          <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://jpapon.github.io/data/pointclouds/cutting_demo_cloud_supervoxels.pcd&psize=3" width="1100" height="750" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="">
          </iframe>
        </section>
        
        <section>
          <section>
            <h2>Local Convexity Segmentation (LCCP)<cite></cite></h2>
            <p>Use a local convexity criterion on adjacency graph edges to split graph.</p> <br>
            <div class="ctr w100">
            <figure class="embed reveal-smooth dark" >
              <img src="data/algorithmic_flow.svg">
              <figcaption style="font-size:1.0em">
                Flow of segmentation: voxels to supervoxels to local convex patches.
              </figcaption>
            </figure>
            </div>
            <br>
            <div class='citation'>
              <footl>
                <footi>Stein, S.; Schoeler, M.; <strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2014-segmentation/">Object Partitioning using Local Convexity,</a> Computer Vision and Pattern Recognition (CVPR) 2014, June 2014.</footi>
              </footl>
            </div>
          </section>
          
          <section>
           <div class="ctr w100">
            <figure class="embed hide-smooth dark" >
              <img src="data/NYU_LCCP_Examples.png">
              <figcaption style="font-size:1.0em">
                LCCP Example Results
              </figcaption>
            </figure>
            </div>
          </section>
          
          <section>
           <div class="ctr w100">
            <figure class="embed hide-smooth dark" >
              <img src="data/lccp_result_images1.svg">
              <figcaption style="font-size:1.0em">
                LCCP Example Results
              </figcaption>
            </figure>
            </div>
          </section>
          
          <section>
           <div class="ctr w100">
            <figure class="embed hide-smooth dark" >
              <img src="data/lccp_result_images2.svg">
              <figcaption style="font-size:1.0em">
                LCCP Example Results
              </figcaption>
            </figure>
            </div>
          </section>
          
          <section>
           <div class="ctr w100">
            <figure class="embed-top hide-smooth dark" >
              <img src="data/OSD_results.png">
              <figcaption style="font-size:1.0em">
                LCCP Comparison on OSD Dataset
              </figcaption>
            </figure>
            <br>
            <figure class="embed-top hide-smooth dark" >
              <img src="data/NYU_results.png">
              <figcaption style="font-size:1.0em">
                LCCP Comparison on NYU Dataset
              </figcaption>
            </figure>
            </div>
          </section>
        </section>       
               
               
        <section>
          <h2>LCCP Segments in a Point Cloud</h2>
          <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://jpapon.github.io/data/pointclouds/cutting_demo_cloud_lccp.pcd&psize=3" width="1100" height="750" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="">
          </iframe>
        </section>
        
        <section>
          <h2>CPC- Constrained Planar Cuts</h2>
          <p>LCCP has a very strict cutting criterion, now we will relax it.</p>
            <div class="ctr w70">
            <figure class="embed reveal-smooth dark" >
              <img src="data/LCCP_Fail.svg">
              <figcaption style="font-size:1.0em">
                LCCP Fails when concavities don't completely isolate parts.
              </figcaption>
            </figure>
            </div>
            <br>
            <div class='citation'>
              <footl>
                <footi>Schoeler, M.; <strong>Papon, J.</strong>;  Wörgötter, F., Constrained Planar Cuts - Object Partitioning for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2015, June 2015.</footi>
              </footl>
            </div>
        </section>
        
        <section>
          <h2>CPC- Constrained Planar Cuts</h2>
          <div class="ctr w100">
            <div class="ctr w70">
              <figure class="embed reveal-smooth dark" >
                <img src="data/Chair.svg">
                <figcaption style="font-size:1.0em">
                  Find planar cuts which score highest - most concavities in support region.
                </figcaption>
              </figure>
            </div>
            <ul>
              <li class="fragment">Extract concavity adjacency graph using Supervoxels, LCCP algorithm</li>
              <li class="fragment">Use weighted RANSAC planar cuts to find highest scoring cuts.</li>
              <li class="fragment">Segmentation proceeds hierarchicaly, terminating once score below threshold. </li>
                
            </ul>
            <div class="ctr w100 fragment">
              <figure class="embed reveal-smooth dark" >
              <img src="data/Steps.svg">
              <figcaption style="font-size:1.0em">
                Cuts proceed recursively until score threshold reached.
              </figcaption>
              </figure>
            </div>
          </div>
        </section>
        
        <section>
          <section>
            <h2>Results on Princeton Segmentation Benchmark</h2>
            <div class="ctr w90">
              <figure class="embed reveal-smooth dark" >
                <img src="data/Princeton_Qualitative.svg">
                <figcaption style="font-size:1.0em">
                  Typical Results on Benchmark Dataset
                </figcaption>
              </figure>
            </div>
              <br>
              <div class='citation'>
                <footl>
                  <footi>X. Chen et al. A benchmark for 3d mesh segmentation.,</a>in Transactions on Graphics, 2009.</footi>
                </footl>
              </div>
            </section>
            
            <section>
            <h2>Directed Cuts</h2>
            <div class="ctr w90">
              <figure class="embed reveal-smooth dark" >
                <img src="data/Directed_vs_undirected_weights.svg">
                <figcaption style="font-size:1.0em">
                  Cuts are weighted by relation to concave connection being cut
                </figcaption>
              </figure>
            </div>
            </section>
            
            <section>
            <h2>Quantitiative Results</h2>
            <div class="ctr w90">
              <figure class="embed reveal-smooth dark" >
                <img src="data/Princeton_Quantitative.svg">
                <figcaption style="font-size:1.0em">
                  Quantitiative Measures on Princeton Dataset
                </figcaption>
              </figure>
            </div>
            </section>
            
        </section>
        
        <section> 
          <h2> Can segment huge full 3D scenes efficiently. </h2>
          <video src="data/movies/city_loop_lores.mp4" height="100%" controls loop class="slideautostart"></video>
        </section>
        
        <section>
          <h2>Overview of Methodology</h2>
          <div>
           <img src="data/Overall_Cropped.svg" width="100%">
           </div>
          <div style="position:absolute;left:785px;top:137px;">
            <video src="data/movies/cutting_noocc_result_png.mp4" height="50%" loop class="slideautostart"></video>
          </div>
        </section>
        
        <!-- SEQUENTIAL POINT CLOUDS -->
        <section>
          <section>
            <h2>Sequential Clouds & Occlusion Reasoning </h2>
            <p> Occlusions appear as “shadows” in rendered point clouds. </p>
            <p> For instance, here the lemon (which we want to keep track of) and much of the table is hidden by the bowl.</p>
            <video src="data/movies/occlusion_demo.mp4" height="50%" controls loop class="slideautostart"></video>
            <p> These blank areas limit our ability to have temporal continuity - object permanence.</p>
          </section>
          
          <section>
            <h2> Pointcloud <em>without</em> Occlusion Reasoning </h2>
            <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://134.76.92.76/data/occlusion_without.pcd&psize=3" width="1100" height="750" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;">
            </iframe>
            <p>Fortunately, we can perform some <span class="fragment highlight-green">low-level reasoning</span> about occlusions.</p>
          </section>
        </section>

        <section>
          <section>
            <h2>Sequentially Updated Octree <cite></cite> </h2>
            <p> If we assume no camera motion, we can reason about why voxels “disappear” </p>
            <p> Check for occlusion by ray-tracing paths from voxel to camera</p>
            <video src="data/movies/occlusion_manip1_other_input_centroids.mp4" width="100%" controls loop class="slideautostart"></video>
            <p> Camera is <strong style="color:red">facing us</strong> from this perspective - notice shadows extend towards the viewer.</p>
            <br><br>

            <div class='citation'>
              <footl>
                <footi>Papon et al., <a href="http://www.jeremiepapon.com/iros-2013-video-segmentation/">Point Cloud Video Object Segmentation using a Persistent Supervoxel World-Model,</a> Intelligent Robots and Systems (IROS) 2013.</footi>
              </footl>
            </div>
          </section>
          
          <section>
            <h2> Demonstration of Occlusion Reasoning </h2>
            <p> Left frame shows input data without occlusion reasoning </p>
            <video src="data/movies/occlusion_comparison.mp4" width="100%" controls loop class="slideautostart"></video>
            <p> Right shows the same input with ray-tracing checks </p>
          </section>
          
          <section>
            <h2> Pointcloud <em>with</em> Occlusion Reasoning </h2>
            <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://134.76.92.76/data/occlusion_with.pcd&psize=3" width="1100" height="750" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;">
            </iframe>
          </section>
        </section>
        
        <section>
          <h2>Overview of Methodology</h2>
          <div>
           <img src="data/Overall_Cropped.svg" width="100%">
           </div>
          <div style="position:absolute;left:785px;top:137px;">
            <video src="data/movies/cutting_noocc_result_png.mp4" height="50%" loop class="slideautostart"></video>
          </div>
        </section>
        
        <!-- PARTICLE FILTERING -->
        <section>
          <h2>Particle filter tracking in Point Clouds</h2>
          <p> Correspondence-Based Particle Filter approach is used. </p>
          <figure class="embed hide-smooth dark" >
              <div style="width:100%">
                  <img src="data/TideModelSV.svg">
                  <figcaption style="font-size:1.0em">
                    Models used for tracking are point clouds, partitioned using supervoxels into strata for sampling.
                  </figcaption>
              </div>
            </figure>
        </section>
      
        <section>
          <h2>Stratified Correspondence Sampling <cite></cite></h2>
          <figure class="embed hide-smooth dark" >
              <div style="width:100%">
                  <img src="data/StratifiedCorrespondences.svg">
                  <figcaption style="font-size:1.0em">
                    Supervoxels are used to choose spatial strata for uniform random sampling.  
                  </figcaption>
              </div>
            </figure>
          <br><br>
          <div class='citation'>
            <footl>
              <footi>Papon et al., <a href="http://www.jeremiepapon.com/wacv-2015-tracking/">Spatially Stratified Correspondence Sampling for Real-Time Point Cloud Tracking,</a> Applications of Computer Vision (WACV), 2015.</footi>
            </footl>
          </div>
        </section>
        
        <section>
          <section>
            <h2> Results in Real Application </h2>
            <video src="data/movies/Demo4Fast.mp4" height="100%" controls loop class="slideautostart"></video>
              <p class="rcred"><a href="http://www.intellact.eu/">IntellAct Project</a></p>
          </section>
          <section>
            <h2> Results in Real Application </h2>
            <video src="data/movies/humandemo.mp4" height="100%" controls loop class="slideautostart"></video>
              <p class="rcred"><a href="http://www.intellact.eu/">IntellAct Project</a></p>
          </section>
        </section>
        
        <section>
          <section>
            <h2> Results on Synthetic Benchmark <cite></cite> </h2>
            <div align="center">
              <video src="data/movies/milk_small.mp4" height="100%" controls loop class="slideautostart"></video>
            </div> <br>
            <div class='citation'>
              <footl>
                <footi>Choi and Christensen,<a href="http://www.cc.gatech.edu/~cchoi/rgbd_obj_tracking.html" “RGB-D> Object Tracking: A Particle Filter Approach on GPU,</a> International Conference on Intelligent Robots and Systems (IROS), 2013.
                </footi>
              </footl>
            </div>
          </section>
          
          <section>
            <h2> Results on Synthetic Benchmark  </h2>
            <div align="center">
              <video src="data/movies/Tide_small.mp4" height="100%" controls loop class="slideautostart"></video>
            </div>
          </section>
          
          <section>
            <h2> Results on Synthetic Benchmark  </h2>
            <div align="center">
              <video src="data/movies/kinect.mp4" height="100%" controls loop class="slideautostart"></video>
            </div>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Results on Synthetic Benchmark</h2>
            <p> 
            <figure class="embed-top reveal-smooth dark" >
                <div style="width:100%">
                  <img src="data/AngularErrorOrangeJuice.svg">
                  <figcaption style="font-size:1.0em">
                      Plot of Displacement Error vs time per frame (ms) averaged across 50 VR Test Runs for different numbers of particles and samples per stratum. 
                  </figcaption>
                </div>
            </figure>
          </section>
          <section>
            <h2>Results on Synthetic Benchmark</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div style="width:100%">
                  <img src="data/DispErrorMilk.svg">
                  <figcaption style="font-size:1.0em">
                      Plot of Rotational Error vs time per frame (ms) averaged across 50 VR Test Runs for different numbers of particles and samples per stratum. 
                  </figcaption>
              </div>
            </figure>
          </section>
        </section>
        
        
        
        <!-- HIERARCHICAL SUPERVOXEL TRACKING -->
        <section>
          <h2> Tracking Low Level Patches - <br>Why Temporal Supervoxels? </h2>
          <p> Tracking low level patches would let us make temporal connections <span class="fragment highlight-green"> <em>without</em> needing to specify a-priori objects.</p>
          <figure class="embed hide-smooth dark" >
                  <img src="data/splitting_objects.svg" >
                  <figcaption style="font-size:1.0em">
                    Splitting objects are problematic if we segment and track using a-priori models. <br> How do we label the pieces?
                  </figcaption>
          </figure>
          <p class="fragment"> We have our low level patch representation - Supervoxels. </p>
          <p class="fragment"> We have en efficient tracking method. </p>
          <p class="fragment" style="font-size:150%; text-align:center"> So, what's the problem? </p>
        </section>
        
        <section>
          <h2> Why can't we just track Supervoxels? </h2>
          <p> Cannot track exclusively at low-level due to the “aperture problem” <cite></cite></p>
          <embed src="data/flash/twosquares.swf" width="90%" height="70%"></embed>
          <p class="rcred"><a href="http://web.mit.edu/persci/demos/Motion&Form/demos/download.html">MIT Perceptual Science Group</a></p>
          <br>
          <div class='citation'>
            <footl>
              <footi>McDermott, et al., <a href="http://web.mit.edu/jhm/www/Pubs/McDermott_Weiss_Adelson_2001_motion_form_beyond_junctions.pdf">Beyond junctions: Nonlocal form contraints on motion interpretation.</a> <em>Perception </em> 2001.</footi>
            </footl>
          </div>
        </section>
        
        <!--section>
          <h2>Cortical Feedback Mechanisms</h2>
          <p>Humans appear to use top-down feedback mechanisms <cite></cite> </p>
          <p>Feedback allows high-level areas to influence low-level vision, even receptive fields </p>
          <div class="ctr" style="height:80%; width:70%">
          <figure class="embed hide-smooth dark" >
                  <img src="data/feedback_connections.jpg" >
                  <figcaption style="font-size:1.0em">
                    Feed-forward and Feedback Mechanisms in the Human Visual Cortex
                  </figcaption>
          </figure>
          </div>
          <br>
          <div class='citation'>
            <footl>
              <footi> Gilbert and Wu Li. <a href="http://www.nature.com/nrn/journal/v14/n5/full/nrn3476.html">Top-down influences on visual processing,</a> <em>Nature Reviews Neuroscience,</em> 2013.</footi>
            </footl>
          </div>
        </section-->
        
        <section>
          <h2> Hierarchical Temporal (super)Voxel Fields (HTVF) </h2>
          <div id="Stage" class="EDGE-3489513"></div>
        </section>

        <section>
          <section>
            <h2> HTVF - Cutting Video 0</h2>
            <video src="data/movies/cutting_render_result.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
          
          <section>
            <h2> HTVF - Cutting Video 1</h2>
            <video src="data/movies/cutting_input_result.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
        </section>
        
        <section>
          <h2> HTVF - Occlusions - Without Voxel Raytracing </h2>
          <video src="data/movies/occlusion1_none_input_result.mp4" height="100%" controls loop class="slideautostart"></video>
        </section>
        
        <section>
          <section>
            <h2> HTVF - Occlusions - With Voxel Raytracing 0</h2>
            <video src="data/movies/occlusion_manip1_other_input_rendering.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
          
          <section>
            <h2> HTVF - Occlusions - With Voxel Raytracing 1 </h2>
            <video src="data/movies/occlusion_manip1_input_result.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
          
          <section>
            <h2> HTVF - Occlusions - With Voxel Raytracing 2 </h2>
            <video src="data/movies/occlusion_manip1_other2_input_voxels.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
          
          <section>
            <h2> HTVF - Occlusions - With Voxel Raytracing 3 </h2>
            <video src="data/movies/occlusion_manip1_other_input_voxels.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
          
          <section>
            <h2> Occlusions - Just Occlusion Filling </h2>
            <video src="data/movies/occlusion_manip1_other_input_centroids.mp4" height="100%" controls loop class="slideautostart"></video>
          </section>
        </section>
        

        <!-- 
        +++++++++++++++++++++
        MACHINE LEARNING STUFF
        +++++++++++++++++++++
        -->
        <section>
          <h2> Current Work - Estimating Pose using deep CNNs </h2>
          <p> Generic 6DoF Pose Estimation is a <em>hard</em> problem </p>
          <ul>
            <li class="fragment">Baseline methods work by finding feature or point correspondences (e.g. ICP)</li>
            <li class="fragment">Much effort has been spent designing model-based approaches for particular classes, but this is time consuming.</li>
          </ul>
          <figure class="embed hide-smooth dark" >
                  <img src="data/bunny.png" >
                  <figcaption style="font-size:1.0em">
                    Can we avoid needing to design features for correspondences, and learn them instead?
                  </figcaption>
          </figure> <br>
          <p class="fragment">Unfortunately, generic pose is very difficult to get training data for.</p>
          <p class="fragment" style="font-size:150%; text-align:center">Fortunately, we can synthesize it!</p>
        </section>
        
        <section>
          <h2>Synthetic Dataset</h2>
          <ul>
            <li class="fragment">Generate data with random poses so we can have accurate ground truth</li>
            <li class="fragment">Use meshes from Princeton ModelNet <cite></cite> - for now, chairs</li>
            <br>
            <li class="fragment">Why Chairs?</li>
            <div class="fragment" >
              <ul style="list-style-type: none">
                <li>Lots of them, aligned with canonical pose</li>
                <li>Can limit degrees of freedom initially</li>
                <li>Lots of intra-class variability, but can still generalize</li>
              </ul>
              <figure class="embed-bottom reveal-smooth dark" >
                <div class="ctr w100">
                    <img src="data/pose/chairs.png">
                    <figcaption style="font-size:1.0em">
                        Chairs. Lots of Chairs.
                    </figcaption>
                </div>
              </figure>
            </div>
          </ul>
          <div class='citation'>
              <footl>
                <footi>Wu, et al. <a href="http://arxiv.org/abs/1406.5670">"3D ShapeNets for 2.5D Object Recognition and Next-Best-View Prediction"</a> <em> arXiv </em> 2014.</footi>
              </footl>
          </div>
        </section>
        
        <section>
            <h2>Synthetic Data</h2>
            <p> Generate data using Blensor Python API and a custom Kinect Sensor Model</p>
            <p> Basic "Room" with 7 randomly (not touching) placed chairs.</p>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/blender.png">
              </div>
            </figure>
        </section>
        
        <section>
          <section>
            <h2>Synthetic Data - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_depth.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_depth.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Synthetic Data - Normals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_nx.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Normals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_ny.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Normals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_nz.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_labels.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_labels.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_depth.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/real_depth.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_nx.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/real_nx.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Signatures - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/depth_sigs.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nx</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/nx_sigs.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>Signatures - ny</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/ny_sigs.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nz</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/nz_sigs.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>LCCP to Generate Proposals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_lccp_boxes.svg">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_labels.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>LCCP to Generate Proposals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_lccp_boxes.svg">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_labels.png">
              </div>
            </figure>
          </section>
        </section>
        
         <section>
            <h2>Network Architecture</h2>
            <p> Pretty standard configuration except for input layer, a la Krizhevsky Imagenet<cite></cite>
            <figure class="embed-top reveal-smooth dark" >
               <div class="ctr w80">
                  <img src="data/pose/Architecture.png">
                  <figcaption style="font-size:1.0em">
                      Network Architecture - 3 Conv. layers, 2 fully connected. 
                  </figcaption>
              </div>
            </figure><br>
            <div class='citation'>
              <footl>
                <footi>Krizhevsky, et al. <a href="http://papers.nips.cc/paper/4824-imagenet">"Imagenet classification with deep convolutional neural networks."</a> <em> NIPS </em> 2012.</footi>
              </footl>
            </div>
        </section>
        
        <section>
            <h2>Loss Function</h2>
            <p> MSE of transformed model points (a la ICP) </p>
            <p>$$ L_i = \frac{1}{ \sum{w_{i,j} } } \sum_{i=1}^{N_o} \sum_{j=1}^{N_m} w_{i,j} \lVert o_i-(\theta m_i + dp) \rVert ^2$$</p>
            <ul>
              <li class="fragment" style="color:green">Direct measure - simply distance between points</li>
              <li class="fragment" style="color:red">Expensive to compute!</li>
              <li class="fragment" style="color:red">Unreliable with occlusions, partial views have local minima, not convex</li>
            </ul>
            <p> MSE of pose directly - euler angles, translation </p>
            <p>$$ L_i = \lVert f - \mathbf{y_i} \rVert ^2$$</p>
             <ul>
              <li class="fragment" style="color:green">Cheap to compute</li>
              <li class="fragment" style="color:green">Will give pose under heavy occlusion <span style="color:red">but no measure of reliability</span></li>
              <li class="fragment" style="color:green">Generalizes well for classes with canonical pose (i.e. most)</li>
              <li class="fragment" style="color:red">Possible scale issues between rotational and translational errors</li>
              <li class="fragment" style="color:red">Regression is harder, less stable than classification</li>
            </ul>
            <p> Binned Pose Binary Log. Regression Classification</p>
            <ul>
              <li class="fragment">CNN predicts a score for a set of bins over the pose space</li>
              <li class="fragment" style="color:green">More stable than regression, easier to train</li>
              <li class="fragment" style="color:red">Only produces coarse pose</li>
            </ul>
            <p>$$ L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j)) $$</p>

            <p> Projected alignment? Other possibilities? </p>
        </section>
       
        <section>
          <section>
              <h2>Early Results</h2>
              <p> Minimal validation error = 0.017175
              <figure class="embed-top reveal-smooth dark" >
                <div class="ctr w80">
                    <img src="data/pose/train_valid_loss.png">
                </div>
              </figure>
          </section>
          <section>
              <h2>Early Results - First Conv layer filters</h2>
              <figure class="embed-top reveal-smooth dark" >
                <div class="ctr w80">
                    <img src="data/pose/responses1.png">
                </div>
              </figure>
          </section>
        </section>
        <section>
          <h2>Summary</h2>
          <div style="font-size: 110%; line-height: 145%">
            <p>We have presented methods for segmenting and tracking streaming point clouds </p>
            <div style="float:right">
              <video src="data/movies/cutting_noocc_result_png.mp4" height="60%" loop class="slideautostart"></video>
            </div>
            <div style="width:65%">
              <p class="fragment"> Our methods:</p>
              <ul>
                <li class="fragment">Can handle occlusions - labels persist </li>
                <li class="fragment">No a-priori assumptions about objects</li>
                <li class="fragment">Can handle rapid movement of people/cameras</li>
                <li class="fragment">Provide stable temporal-supervoxels and segments that can be used for learning</li>
              </ul>
            </div>
          </div>
          <p class="fragment"> Presented ongoing research into applying visual learning to point cloud data. </p> <br>
          <div class="fragment">
            <div style="width:50%; float:left">
              <p>Other Work</p>
              <ul>
                <li> Oculus vision GUI </li>
                <li> All algorithms have been released as Open Source </li>
                <li> 2D Tracking and Segmentation using Particle Filters </li>
              </ul>
            </div>
            <div style="width:40%; float:right">
              <figure class="embed hide-smooth dark" >
                  <img src="data/oculus.png">
                  <figcaption style="font-size:0.7em">
                      Oculus Vision GUI
                  </figcaption>
              </figure>
            </div>
          </div>
          
        </section>
        
        <section>
          <h2>Outlook and Future Work</h2>
          <div style="font-size: 110%; line-height: 145%">
            <p> Many opportunities exist now that we have low-level temporal connections </p>
            <ul> 
              <li class="fragment">Bootstrap learning on video use temporal supervoxels as ground truth to learn features for segmenting video </li>
              <li class="fragment">Use temporal supervoxels as input feature, learn to segment out objects, class-free</li>
              
              <li class="fragment"> Secondary input stream in CNN architecture - object to estimate pose of in scene, how well object matches observation.</li>

              <li class="fragment"> Sensor pose - use synthetic data to train odometry features</li>
              <li class="fragment">Dynamic level of detail & attention for Supervoxels</li>
                <ul class="fragment" style="list-style-type: none">
                  <li>Less samples on large uniform surfaces </li>
                  <li>More samples on small irregular areas </li>
                </ul>
            </ul>
          </div>
          <br>
          <div style="width:50%; float:left">
            <figure class="embed hide-smooth dark" >
                <img src="data/icub_blocks.jpg">
                <figcaption style="font-size:1.0em">
                    Bootstrapping Visual Understanding...
                </figcaption>
            </figure>
            <p class="rcred"><a href="http://www.icub.org/">iCub</a></p>
          </div>
          <div style="width:50%; float:right">
          <br>
            <figure class="fragment embed hide-smooth dark" data-fragment-index="1"  >
                <img src="data/icub_block_evil.jpg">
                <figcaption style="font-size:1.0em">
                    So they never lose track of you.
                </figcaption>
            </figure>
            <p class="rcred"><a href="http://www.icub.org/">iCub</a></p>
          </div>
        </section>
        
        <section>
          <h2>Questions?</h2>
          <h4 align="left">Selected Related Publications</h4>
          <ul style="font-size: 60%; line-height: 135%; width:50%; float:left">
            <li>Schoeler, M.;<strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/">Constrained Planar Cuts - Object Partitioning for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2015, June 2015.</li>
            <li><strong>Papon, J.</strong>; Schoeler, M.; Wörgötter, F., <a href="http://www.jeremiepapon.com/wacv-2015-tracking/">Spatially Stratified Correspondence Sampling for Real-Time Point Cloud Tracking,</a> Applications of Computer Vision (WACV), 2015 IEEE International Conference on, Jan. 2015.</li>
            <br style="line-height:150%">
            <li>Stein, S.; Schoeler, M.; <strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2014-segmentation/">Object Partitioning using Local Convexity,</a> Computer Vision and Pattern Recognition (CVPR) 2014, June 2014. </li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>;  Kulvicius, T.; Aksoy, E.; Wörgötter, F. <a href="http://www.jeremiepapon.com/iros-2013-video-segmentation/">Point Cloud Video Object Segmentation using a Persistent Supervoxel World-Model,</a> Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on, Nov. 2013.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Schoeler, M.; Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2013, June 2013.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Wörgötter, F., <a href="http://www.jeremiepapon.com/eccv-2012-ws-2d-video-segmentation/">Occlusion Handling in Video Segmentation via Predictive Feedback,</a> European Conference on Computer Vision (ECCV) 2012, Workshops and Demonstrations, Oct. 2012.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Aksoy, E.; Wörgötter, F., <a href="http://www.jeremiepapon.com/wacv-2012-oculus-system/">A modular system architecture for online parallel vision pipelines,</a> Applications of Computer Vision (WACV) 2012, Jan. 2012.</li>
          </ul>
          <div style="width:40%; float:right">
            <img src="data/point-cloud-library-logo.png">
            <img src="data/theano_logo.png">
          </div>
        </section>


    <!-- End of slides. -->

    <script src="reveal/lib/js/head.min.js"></script>
    <script src="reveal/js/reveal.min.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script>
      Reveal.initialize({
        width: 1200,
        height: 800,
        controls: true,
        progress: true,
        history: true,
        center: true,
        keyboard: true,
        overview: true,
        
        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal/lib/js/classList.js',
            condition: function() { return !document.body.classList; }},
          { src: 'reveal/plugin/markdown/marked.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/markdown/markdown.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/highlight/highlight.js', async: true,
            callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal/plugin/notes/notes.js', async: true,
            condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
          { src: 'pdfimgs.js', async: true },
          { src: 'slideautostart.js', async: true },
        ],
      });

      // Only load SDO Data page if we go to that slide
      Reveal.addEventListener ('sdodataslide', function () {
        console.log('sdodataslide');
        document.querySelector ('#sdoiframe').src = 'http://sdo.gsfc.nasa.gov/data/'
      });
    </script>
  </body>
</html>
